<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>Ch2 预备知识 | 望秋弥茂</title><meta name="keywords" content="Deep Learning"><meta name="author" content="望秋弥茂"><meta name="copyright" content="望秋弥茂"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="Ch2 预备知识"><meta name="application-name" content="Ch2 预备知识"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="Ch2 预备知识"><meta property="og:url" content="https://www.wangqiumimao.ink/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/index.html"><meta property="og:site_name" content="望秋弥茂"><meta property="og:description" content="2.1数据操作2.1.5 节省内存 ⽤X[:] &amp;#x3D; X + Y或X +&amp;#x3D; Y来减少操作的内存开销12X[:] &amp;#x3D; X + YX +&amp;#x3D; Y  2.2 数据预处理2.2.1 读取数据集123456789101112# 写入数据import osos.makedirs(os.path."><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231207195728.png"><meta property="article:author" content="望秋弥茂"><meta property="article:tag" content="机器人"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231207195728.png"><meta name="description" content="2.1数据操作2.1.5 节省内存 ⽤X[:] &amp;#x3D; X + Y或X +&amp;#x3D; Y来减少操作的内存开销12X[:] &amp;#x3D; X + YX +&amp;#x3D; Y  2.2 数据预处理2.2.1 读取数据集123456789101112# 写入数据import osos.makedirs(os.path."><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/zahir-w/picgo/%E7%B2%BE%E7%81%B5%E7%90%83.svg"><link rel="canonical" href="https://www.wangqiumimao.ink/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"><link rel="preconnect" href="//cdn.cbd.int"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: undefined,
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: {"mode":"api","api":"https://img2color-go.vercel.app/api?img=","cover_change":false},
  authorStatus: {"skills":["🤖️ 搞机！搞机！搞机！","🔍 这是什么？摸一下！"]},
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: 望秋弥茂","link":"链接: ","source":"来源: 望秋弥茂","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: '望秋弥茂',
  title: 'Ch2 预备知识',
  postAI: '',
  pageFillDescription: '2.1数据操作, 2.1.5 节省内存, 2.2 数据预处理, 2.2.1 读取数据集, 2.2.2 处理缺失值, 2.2.3 转换为张量格式, 2.3 线性代数, 2.3.2 向量, 2.3.3 矩阵, 2.3.4 张量, 2.3.5 张量算法的基本性质, 2.3.6 降维, 非降维求和, 2.3.7 点积, 2.3.8 矩阵-向量积, 2.3.9 矩阵-矩阵乘法, 2.3.10 范数, 练习, 1.证明⼀个矩阵 $A$ 的转置的转置是A即 $(A^T)^T x3D A$, 对于每一个元素定义易证, 2.给出两个矩阵 $A$ 和 $B$ 证明它们转置的和等于它们和的转置即$A^T+B^Tx3D(A+B)^T$, 对于每一个元素定义易证, 3.给定任意⽅阵 $A$  $A+A^T$ 总是对称的吗?为什么?, 对于每一个元素定义易证, 4.本节中定义了形状(2 3 4)的张量X。len(X)的输出结果是什么？, 5.对于任意形状的张量X len(X)是否总是对应于X特定轴的⻓度?这个轴是什么?, 对应shape的第一个轴的数目axisx3D0, 6.运⾏Ax2FA.sum(axisx3D1)看看会发⽣什么。请分析⼀下原因？, 7.考虑⼀个具有形状(2 3 4)的张量在轴0、1、2上的求和输出是什么形状?, 三维张量可以视作一个空间矩阵然后分别沿着axis（第一个0依次递增）进行降维求和减少的是对应axis方向的维度。, 8.为linalg.norm函数提供3个或更多轴的张量并观察其输出。对于任意形状的张量这个函数计算得到什么?, 本质就是在求每个元素的平方和的平方根结果。, 2.4 微积分, 将拟合模型的任务分解为两个关键问题：, 2.4.1 导数与微分, 2.4.2 偏导数, 2.4.3 梯度, 2.4.4 链式法则, 练习, 1.绘制函数$y x3D f(x) x3D x^3 - \frac{1}{x}$和其在$x x3D 1$处切线的图像。, 2.求函数$f(\mathbf{x}) x3D 3x_1^2 + 5e^{x_2}$的梯度。, 3.函数$f(\mathbf{x}) x3D |\mathbf{x}|_2$的梯度是什么？, 二范数定义如下, 证明如下, 4.尝试写出函数$u x3D f(x y z)$其中$x x3D x(a b)$$y x3D y(a b)$$z x3D z(a b)$的链式法则。, 2.5 自动微分, 2.5.0 深度学习框架, 2.5.1 ⼀个简单的例⼦, 2.5.2 ⾮标量变量的反向传播, 2.5.3 分离计算, 将某些计算移动到记录的计算图之外, 2.5.4 Python控制流的梯度计算, 练习, 会报错RunTimeError, 会报错RunTimeError, 2.6 概率, 2.6.1 基本概率论, 概率论公理, 随机变量, 2.6.2 处理多个随机变量, 联合概率 (Joint Probability), 条件概率 (Conditional Probability), 乘法法则 (Multiplication Rule), ⻉叶斯定理 (Bayes’ Theorem), 边缘化 (Marginalization), 独⽴性 (Independence), 2.6.3 期望和⽅差, 期望 (Expectation), 方差 (Variance), 练习数据操作节省内存或来减少操作的内存开销数据预处理读取数据集写入数据列名每表个数据样本读取数据集处理缺失值同列的均值替换项自动将的将此列转换为非的值以及取值为转换为张量格式都是数值条目线性代数向量访问索引元素长度维度张量的维度用来表示张量具有的轴数矩阵更改维度转置张量个的矩阵张量算法的基本性质降维结果和相同也有一样的语法非降维求和在对每进求和后仍保持两个轴直接除可以获得每一行自己的平均数元素该行总和某个轴进行累积求和点积矩阵向量积矩阵矩阵乘法标准矩阵乘法与积点对点乘积不同范数范数中常常省略下标也就是说等同于范数范数定义范数范数满向量范数的所有性质它就像是矩阵形向量的范数练习证明个矩阵的转置的转置是即对于每一个元素定义易证给出两个矩阵和证明它们转置的和等于它们和的转置即对于每一个元素定义易证给定任意阵总是对称的吗为什么对于每一个元素定义易证本节中定义了形状的张量的输出结果是什么对于任意形状的张量是否总是对应于特定轴的度这个轴是什么对应的第一个轴的数目运看看会发什么请分析下原因矩阵除以一维张量要看矩阵列数是否等于张量的元素个数也可以看作是列数是否相等如果相等就可以实现将矩阵该列的所有元素除以对应列的张量元素否则会报错考虑个具有形状的张量在轴上的求和输出是什么形状三维张量可以视作一个空间矩阵然后分别沿着第一个依次递增进行降维求和减少的是对应方向的维度为函数提供个或更多轴的张量并观察其输出对于任意形状的张量这个函数计算得到什么本质就是在求每个元素的平方和的平方根结果微积分将拟合模型的任务分解为两个关键问题优化模型拟合观测数据的过程泛化数学原理和实践者的智慧能够指导我们成出有效性超出于训练的数据集本的模型导数与微分是个特殊的标记会将对应的函数类或语句保存在包中函数于设置由成图表的轴的属性设置的轴定义个函数来简洁地绘制多条曲线绘制数据点如果有个轴输出偏导数对于偏导数的表示以下是等价的梯度设函数的输入是一个维向量并且输出是一个标量函数相对于的梯度是一个包含个偏导数的向量其中通常在没有歧义时被取代假设为维向量在微分多元函数时经常使用以下规则对于所有都有对于所有都有对于所有都有同样对于任何矩阵都有链式法则练习绘制函数和其在处切线的图像求函数的梯度函数的梯度是什么二范数定义如下证明如下尝试写出函数其中的链式法则自动微分深度学习框架通过动计算导数即动微分来加快求导根据设计好的模型系统会构建个计算图来跟踪计算是哪些数据通过哪些操作组合起来产输出动微分使系统能够随后反向传播梯度这反向传播意味着跟踪整个计算图填充关于每个参数的偏导数个简单的例等价于默认值是调用反向传播函数对于分量的梯度标量变量的反向传播在默认情况下会累积梯度需要清除之前的值分离计算将某些计算移动到记录的计算图之外假设是作为的函数计算的则是作为和的函数计算的想象下我们想计算关于的梯度但由于某种原因希望将视为个常数分离来返回个新变量该变量与具有相同的值但丢弃计算图中如何计算的任何信息梯度不会向后流经到分离计算设置变量但仅使用值不考虑如何计算的从而避免的反向传播控制流的梯度计算使动微分的个好处是即使构建函数的计算图需要通过控制流例如条件循环或任意函数调仍然可以计算得到的变量的梯度练习为什么计算阶导数阶导数的开销要更二阶相当于再复用一阶导数的计算在运反向传播函数之后即再次运它看看会发什么会报错两次在控制流的例中我们计算关于的导数如果将变量更改为随机向量或矩阵会发什么会报错只支持标量的重新设计个求控制流梯度的例运并分析结果暂无使绘制和的图像其中后者不使用概率基本概率论投骰子的概率都是次的模拟结果次的模拟结果概率论公理概率非负性互斥事件概率为单个事件发生概率之和随机变量离散随机变量连续随机变量处理多个随机变量联合概率条件概率乘法法则叶斯定理边缘化独性期望和差期望离散随机变量连续随机变量方差练习进行组实验每组抽取个样本改变和观察和分析实验结果中心极限定理给定两个概率为和的事件计算和的上限和下限提示使用友元图来展示这些情况假设我们有一系列随机变量例如和其中只依赖于而只依赖于能简化联合概率吗提示这是一个马尔可夫链在节中第一个测试更准确为什么不运行第一个测试两次而是同时运行第一个和第二个测试也许是成本问题只需要第二次的不精准测试也可以大幅度提高推理在阳性反应下患者真实患病的概率同时可以避免同种测试方法可能存在的误差和不为人知的错误',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-15 12:04:55',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/font.css"<meta name="generator" content="Hexo 7.0.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/blogavatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">望秋弥茂</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 幽记</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-shounahe"></use></svg><span> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-shaixuan"></use></svg><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-biaoqian1"></use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 此间</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-taiyang"></use></svg><span> 友人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-wodeyouxiang"></use></svg><span> 夜话</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 观微</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/note/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-icon-rgb_jiqixuexisuanfayinqing"></use></svg><span> 格物</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-sf"></use></svg><span> 致知</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-dianzan"></use></svg><span> 本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xiangji"></use></svg><span> 光影</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Deep-Learning/" style="font-size: 1.05rem;">Deep Learning<sup>1</sup></a><a href="/tags/Imitation-Learning/" style="font-size: 1.05rem;">Imitation Learning<sup>1</sup></a><a href="/tags/Reinforcement-Learning/" style="font-size: 1.05rem;">Reinforcement Learning<sup>3</sup></a><a href="/tags/RobotMail/" style="font-size: 1.05rem;">RobotMail<sup>4</sup></a><a href="/tags/SNN/" style="font-size: 1.05rem;">SNN<sup>1</sup></a><a href="/tags/Simulator/" style="font-size: 1.05rem;">Simulator<sup>3</sup></a><a href="/tags/Slide-Detection/" style="font-size: 1.05rem;">Slide Detection<sup>1</sup></a><a href="/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/" style="font-size: 1.05rem;">博客搭建<sup>1</sup></a><a href="/tags/%E5%B0%8F%E7%BA%B8%E6%9D%A1/" style="font-size: 1.05rem;">小纸条<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">十二月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">十一月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/01/"><span class="card-archive-list-date">一月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/08/"><span class="card-archive-list-date">八月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/02/"><span class="card-archive-list-date">二月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/Deep-Learning/" itemprop="url">Deep Learning</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/Deep-Learning/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Deep Learning</span></a></span></div></div><h1 class="post-title" itemprop="name headline">Ch2 预备知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2023-12-07T11:37:28.000Z" title="发表于 2023-12-07 19:37:28">2023-12-07</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2023-12-15T04:04:55.384Z" title="更新于 2023-12-15 12:04:55">2023-12-15</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-pv-cv" id="" data-flag-title="Ch2 预备知识"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为冰岛"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>冰岛</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231207195728.png"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://www.wangqiumimao.ink/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"><header><a class="post-meta-categories" href="/categories/Deep-Learning/" itemprop="url">Deep Learning</a><a href="/tags/Deep-Learning/" tabindex="-1" itemprop="url">Deep Learning</a><h1 id="CrawlerTitle" itemprop="name headline">Ch2 预备知识</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">望秋弥茂</span><time itemprop="dateCreated datePublished" datetime="2023-12-07T11:37:28.000Z" title="发表于 2023-12-07 19:37:28">2023-12-07</time><time itemprop="dateCreated datePublished" datetime="2023-12-15T04:04:55.384Z" title="更新于 2023-12-15 12:04:55">2023-12-15</time></header><h1 id="2-1数据操作"><a href="#2-1数据操作" class="headerlink" title="2.1数据操作"></a>2.1数据操作</h1><h2 id="2-1-5-节省内存"><a href="#2-1-5-节省内存" class="headerlink" title="2.1.5 节省内存"></a>2.1.5 节省内存</h2><ul>
<li>⽤X[:] &#x3D; X + Y或X +&#x3D; Y来<strong>减少操作的内存开销</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[:] = X + Y</span><br><span class="line">X += Y</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h1><h2 id="2-2-1-读取数据集"><a href="#2-2-1-读取数据集" class="headerlink" title="2.2.1 读取数据集"></a>2.2.1 读取数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写入数据</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>) <span class="comment"># ../data/house_tiny.csv</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">	f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>) <span class="comment"># 列名</span></span><br><span class="line">	f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>) <span class="comment"># 每⾏表⽰⼀个数据样本</span></span><br><span class="line">	f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据集</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br></pre></td></tr></table></figure>

<h2 id="2-2-2-处理缺失值"><a href="#2-2-2-处理缺失值" class="headerlink" title="2.2.2 处理缺失值"></a>2.2.2 处理缺失值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = inputs.fillna(inputs.mean()) <span class="comment"># 同⼀列的均值替换“NaN”项</span></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>) <span class="comment"># 自动将input的将此列转换为非NaN的值以及NaN，取值为0，1</span></span><br></pre></td></tr></table></figure>
<h2 id="2-2-3-转换为张量格式"><a href="#2-2-3-转换为张量格式" class="headerlink" title="2.2.3 转换为张量格式"></a>2.2.3 转换为张量格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values) <span class="comment"># input,output都是数值条目</span></span><br></pre></td></tr></table></figure>
<h1 id="2-3-线性代数"><a href="#2-3-线性代数" class="headerlink" title="2.3 线性代数"></a>2.3 线性代数</h1><h2 id="2-3-2-向量"><a href="#2-3-2-向量" class="headerlink" title="2.3.2 向量"></a>2.3.2 向量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x[<span class="number">3</span>] <span class="comment"># 访问索引元素</span></span><br><span class="line"><span class="built_in">len</span>(x) <span class="comment"># 长度</span></span><br><span class="line">x.shape <span class="comment"># 维度</span></span><br></pre></td></tr></table></figure>
<ul>
<li>张量的维度用来表示张量具有的轴数</li>
</ul>
<h2 id="2-3-3-矩阵"><a href="#2-3-3-矩阵" class="headerlink" title="2.3.3 矩阵"></a>2.3.3 矩阵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>) <span class="comment"># 更改维度5×4</span></span><br><span class="line">A.T <span class="comment"># 转置</span></span><br></pre></td></tr></table></figure>
<h2 id="2-3-4-张量"><a href="#2-3-4-张量" class="headerlink" title="2.3.4 张量"></a>2.3.4 张量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 2个3×4的矩阵</span></span><br></pre></td></tr></table></figure>
<h2 id="2-3-5-张量算法的基本性质"><a href="#2-3-5-张量算法的基本性质" class="headerlink" title="2.3.5 张量算法的基本性质"></a>2.3.5 张量算法的基本性质</h2><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231206163412.png" alt="xxx" style="zoom:50%;" /></center>

<h2 id="2-3-6-降维"><a href="#2-3-6-降维" class="headerlink" title="2.3.6 降维"></a>2.3.6 降维</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = (tensor([[ 0., 1., 2., 3.],</span><br><span class="line">			 [ 4., 5., 6., 7.],</span><br><span class="line">			 [ 8., 9., 10., 11.],</span><br><span class="line">			 [12., 13., 14., 15.],</span><br><span class="line">			 [16., 17., 18., 19.]]),</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([40., 45., 50., 55.]), torch.Size([4]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>]) <span class="comment"># 结果和A.sum()相同</span></span><br><span class="line">A.mean(axis=[<span class="number">0</span>, <span class="number">1</span>]) <span class="comment"># mean也有一样的语法</span></span><br></pre></td></tr></table></figure>
<h3 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 6.],</span><br><span class="line">		[22.],</span><br><span class="line">		[38.],</span><br><span class="line">		[54.],</span><br><span class="line">		[70.]])</span><br></pre></td></tr></table></figure>
<ul>
<li>sum_A在对每⾏进⾏求和后仍保持两个轴, 直接除可以获得每一行自己的平均数，元素&#x2F;该行总和<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A / sum_A </span><br></pre></td></tr></table></figure></li>
<li>某个轴进行累积求和<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0., 1., 2., 3.],</span><br><span class="line">		[ 4., 6., 8., 10.],</span><br><span class="line">		[12., 15., 18., 21.],</span><br><span class="line">		[24., 28., 32., 36.],</span><br><span class="line">		[40., 45., 50., 55.]])</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="2-3-7-点积"><a href="#2-3-7-点积" class="headerlink" title="2.3.7 点积"></a>2.3.7 点积</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.dot(x, y)</span><br><span class="line">torch.<span class="built_in">sum</span>(x * y)</span><br></pre></td></tr></table></figure>
<h2 id="2-3-8-矩阵-向量积"><a href="#2-3-8-矩阵-向量积" class="headerlink" title="2.3.8 矩阵-向量积"></a>2.3.8 矩阵-向量积</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.shape, x.shape, torch.mv(A, x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([5, 4]), torch.Size([4]), tensor([ 14., 38., 62., 86., 110.]))</span><br></pre></td></tr></table></figure>
<h2 id="2-3-9-矩阵-矩阵乘法"><a href="#2-3-9-矩阵-矩阵乘法" class="headerlink" title="2.3.9 矩阵-矩阵乘法"></a>2.3.9 矩阵-矩阵乘法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(A, B) <span class="comment"># 标准矩阵乘法，与Hadamard积（点对点乘积）不同</span></span><br></pre></td></tr></table></figure>
<h2 id="2-3-10-范数"><a href="#2-3-10-范数" class="headerlink" title="2.3.10 范数"></a>2.3.10 范数</h2><ul>
<li><strong>$L_2$ 范数</strong>中常常省略下标2，也就是说 $||x||$ 等同于 $||x||_2$<br>$$<br>|\mathbf{x}|<em>2&#x3D;\sqrt{\sum</em>{i&#x3D;1}^n x_i^2}<br>$$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure>

<ul>
<li>$L_1$ 范数<br>$$<br>|\mathbf{x}|<em>1&#x3D;\sum</em>{i&#x3D;1}^n\left|x_i\right|<br>$$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<ul>
<li>$L_p$ <strong>范数定义</strong><br>$$<br>|\mathbf{x}|<em>p&#x3D;\left(\sum</em>{i&#x3D;1}^n\left|x_i\right|^p\right)^{1 &#x2F; p}<br>$$</li>
<li><strong>Frobenius 范数</strong><ul>
<li>Frobenius范数满⾜向量范数的所有性质，它就像是矩阵形向量的 $L_2$ 范数。<br>$$<br>|\mathbf{X}|<em>F&#x3D;\sqrt{\sum</em>{i&#x3D;1}^m \sum_{j&#x3D;1}^n x_{i j}^2}<br>$$</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(torch.ones((<span class="number">4</span>, <span class="number">9</span>)))</span><br></pre></td></tr></table></figure>
<h1 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h1><h3 id="1-证明⼀个矩阵-A-的转置的转置是A，即-A-T-T-A"><a href="#1-证明⼀个矩阵-A-的转置的转置是A，即-A-T-T-A" class="headerlink" title="1.证明⼀个矩阵 $A$ 的转置的转置是A，即 $(A^T)^T &#x3D; A$"></a>1.证明⼀个矩阵 $A$ 的转置的转置是A，即 $(A^T)^T &#x3D; A$</h3><ul>
<li><h4 id="对于每一个元素定义易证"><a href="#对于每一个元素定义易证" class="headerlink" title="对于每一个元素定义易证"></a>对于每一个元素定义易证</h4></li>
</ul>
<h3 id="2-给出两个矩阵-A-和-B-，证明“它们转置的和”等于“它们和的转置”，即-A-T-B-T-A-B-T"><a href="#2-给出两个矩阵-A-和-B-，证明“它们转置的和”等于“它们和的转置”，即-A-T-B-T-A-B-T" class="headerlink" title="2.给出两个矩阵 $A$ 和 $B$ ，证明“它们转置的和”等于“它们和的转置”，即$A^T+B^T&#x3D;(A+B)^T$"></a>2.给出两个矩阵 $A$ 和 $B$ ，证明“它们转置的和”等于“它们和的转置”，即$A^T+B^T&#x3D;(A+B)^T$</h3><ul>
<li><h4 id="对于每一个元素定义易证-1"><a href="#对于每一个元素定义易证-1" class="headerlink" title="对于每一个元素定义易证"></a>对于每一个元素定义易证</h4></li>
</ul>
<h3 id="3-给定任意⽅阵-A-，-A-A-T-总是对称的吗-为什么"><a href="#3-给定任意⽅阵-A-，-A-A-T-总是对称的吗-为什么" class="headerlink" title="3.给定任意⽅阵 $A$ ， $A+A^T$ 总是对称的吗?为什么?"></a>3.给定任意⽅阵 $A$ ， $A+A^T$ 总是对称的吗?为什么?</h3><ul>
<li><h4 id="对于每一个元素定义易证-2"><a href="#对于每一个元素定义易证-2" class="headerlink" title="对于每一个元素定义易证"></a>对于每一个元素定义易证</h4></li>
</ul>
<h3 id="4-本节中定义了形状-2-3-4-的张量X。len-X-的输出结果是什么？"><a href="#4-本节中定义了形状-2-3-4-的张量X。len-X-的输出结果是什么？" class="headerlink" title="4.本节中定义了形状(2; 3; 4)的张量X。len(X)的输出结果是什么？"></a>4.本节中定义了形状(2; 3; 4)的张量X。len(X)的输出结果是什么？</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(X))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure>
<h3 id="5-对于任意形状的张量X-len-X-是否总是对应于X特定轴的⻓度-这个轴是什么"><a href="#5-对于任意形状的张量X-len-X-是否总是对应于X特定轴的⻓度-这个轴是什么" class="headerlink" title="5.对于任意形状的张量X, len(X)是否总是对应于X特定轴的⻓度?这个轴是什么?"></a>5.对于任意形状的张量X, len(X)是否总是对应于X特定轴的⻓度?这个轴是什么?</h3><ul>
<li><h4 id="对应shape的第一个轴的数目，axis-0"><a href="#对应shape的第一个轴的数目，axis-0" class="headerlink" title="对应shape的第一个轴的数目，axis&#x3D;0"></a>对应shape的第一个轴的数目，axis&#x3D;0</h4></li>
</ul>
<h3 id="6-运⾏A-A-sum-axis-1-，看看会发⽣什么。请分析⼀下原因？"><a href="#6-运⾏A-A-sum-axis-1-，看看会发⽣什么。请分析⼀下原因？" class="headerlink" title="6.运⾏A&#x2F;A.sum(axis&#x3D;1)，看看会发⽣什么。请分析⼀下原因？"></a>6.运⾏A&#x2F;A.sum(axis&#x3D;1)，看看会发⽣什么。请分析⼀下原因？</h3><ul>
<li>矩阵除以一维张量，要看矩阵列数是否等于张量的元素个数（也可以看作是列数）是否相等，如果相等就可以实现，将矩阵该列的所有元素除以对应列的张量元素。否则会报错<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A, A.shape)</span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">0</span>), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(A/A.<span class="built_in">sum</span>(axis=<span class="number">0</span>), (A/A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)).shape)</span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">1</span>), A.<span class="built_in">sum</span>(axis=<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>), A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(A/A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>), (A/A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)).shape)</span><br><span class="line"><span class="built_in">print</span>(A/A.<span class="built_in">sum</span>(axis=<span class="number">1</span>), (A/A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)).shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0, 1, 2, 3], </span><br><span class="line">		[ 4, 5, 6, 7], </span><br><span class="line">		[ 8, 9, 10, 11], </span><br><span class="line">		[12, 13, 14, 15], </span><br><span class="line">		[16, 17, 18, 19]]) torch.Size([5, 4]) </span><br><span class="line">tensor([40, 45, 50, 55]) torch.Size([4]) </span><br><span class="line">tensor([[0.0000, 0.0222, 0.0400, 0.0545], </span><br><span class="line">		[0.1000, 0.1111, 0.1200, 0.1273], </span><br><span class="line">		[0.2000, 0.2000, 0.2000, 0.2000], </span><br><span class="line">		[0.3000, 0.2889, 0.2800, 0.2727], </span><br><span class="line">		[0.4000, 0.3778, 0.3600, 0.3455]]) torch.Size([5, 4]) </span><br><span class="line">tensor([ 6, 22, 38, 54, 70]) torch.Size([5]) </span><br><span class="line">tensor([[ 6], [22], [38], [54], [70]]) torch.Size([5, 1]) </span><br><span class="line">tensor([[0.0000, 0.1667, 0.3333, 0.5000], </span><br><span class="line">		[0.1818, 0.2273, 0.2727, 0.3182], </span><br><span class="line">		[0.2105, 0.2368, 0.2632, 0.2895], </span><br><span class="line">		[0.2222, 0.2407, 0.2593, 0.2778], </span><br><span class="line">		[0.2286, 0.2429, 0.2571, 0.2714]]) torch.Size([5, 4])</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="7-考虑⼀个具有形状-2-3-4-的张量，在轴0、1、2上的求和输出是什么形状"><a href="#7-考虑⼀个具有形状-2-3-4-的张量，在轴0、1、2上的求和输出是什么形状" class="headerlink" title="7.考虑⼀个具有形状(2; 3; 4)的张量，在轴0、1、2上的求和输出是什么形状?"></a>7.考虑⼀个具有形状(2; 3; 4)的张量，在轴0、1、2上的求和输出是什么形状?</h3><ul>
<li><h4 id="三维张量可以视作一个空间矩阵，然后分别沿着axis（第一个0，依次递增）进行降维求和，减少的是对应axis方向的维度。"><a href="#三维张量可以视作一个空间矩阵，然后分别沿着axis（第一个0，依次递增）进行降维求和，减少的是对应axis方向的维度。" class="headerlink" title="三维张量可以视作一个空间矩阵，然后分别沿着axis（第一个0，依次递增）进行降维求和，减少的是对应axis方向的维度。"></a>三维张量可以视作一个<strong>空间矩阵</strong>，然后分别沿着axis（第一个0，依次递增）进行<strong>降维求和，减少的是对应axis方向的维度。</strong></h4></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X, X.shape)</span><br><span class="line"><span class="built_in">print</span>(X.<span class="built_in">sum</span>(axis=<span class="number">0</span>), X.<span class="built_in">sum</span>(axis=<span class="number">0</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(X.<span class="built_in">sum</span>(axis=<span class="number">1</span>), X.<span class="built_in">sum</span>(axis=<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(X.<span class="built_in">sum</span>(axis=<span class="number">2</span>), X.<span class="built_in">sum</span>(axis=<span class="number">2</span>).shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 0, 1, 2, 3], </span><br><span class="line">		 [ 4, 5, 6, 7], </span><br><span class="line">		 [ 8, 9, 10, 11]], </span><br><span class="line">		 </span><br><span class="line">		[[12, 13, 14, 15], </span><br><span class="line">		 [16, 17, 18, 19], </span><br><span class="line">		 [20, 21, 22, 23]]]) torch.Size([2, 3, 4]) </span><br><span class="line">tensor([[12, 14, 16, 18], </span><br><span class="line">		[20, 22, 24, 26], </span><br><span class="line">		[28, 30, 32, 34]]) torch.Size([3, 4]) </span><br><span class="line">tensor([[12, 15, 18, 21], </span><br><span class="line">		[48, 51, 54, 57]]) torch.Size([2, 4]) </span><br><span class="line">tensor([[ 6, 22, 38], </span><br><span class="line">		[54, 70, 86]]) torch.Size([2, 3])</span><br></pre></td></tr></table></figure>
<h3 id="8-为linalg-norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么"><a href="#8-为linalg-norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么" class="headerlink" title="8.为linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?"></a>8.为linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?</h3><ul>
<li><h4 id="本质就是在求每个元素的平方和的平方根结果。"><a href="#本质就是在求每个元素的平方和的平方根结果。" class="headerlink" title="本质就是在求每个元素的平方和的平方根结果。"></a>本质就是在求<strong>每个元素的平方和的平方根结果。</strong></h4></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">X = X.numpy()</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(np.linalg.norm(X))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[[1. 1. 1. 1.] </span><br><span class="line">  [1. 1. 1. 1.] </span><br><span class="line">  [1. 1. 1. 1.]] </span><br><span class="line">  </span><br><span class="line"> [[1. 1. 1. 1.] </span><br><span class="line">  [1. 1. 1. 1.] </span><br><span class="line">  [1. 1. 1. 1.]]] </span><br><span class="line">4.8989797 </span><br><span class="line">4.898979485566356</span><br></pre></td></tr></table></figure>


<h1 id="2-4-微积分"><a href="#2-4-微积分" class="headerlink" title="2.4 微积分"></a>2.4 微积分</h1><ul>
<li><h3 id="将拟合模型的任务分解为两个关键问题："><a href="#将拟合模型的任务分解为两个关键问题：" class="headerlink" title="将拟合模型的任务分解为两个关键问题："></a>将拟合模型的任务分解为两个关键问题：</h3><ol>
<li>#优化（optimization） ：⽤模型拟合观测数据的过程</li>
<li>#泛化（generalization） ：数学原理和实践者的智慧，能够指导我们⽣成出有效性超出⽤于训练的数据集本⾝的模型。</li>
</ol>
</li>
</ul>
<h2 id="2-4-1-导数与微分"><a href="#2-4-1-导数与微分" class="headerlink" title="2.4.1 导数与微分"></a>2.4.1 导数与微分</h2><ul>
<li><strong>#@save</strong>是⼀个特殊的标记，会将对应的函数、类或语句保存在d2l包中</li>
<li><strong>set_axes函数</strong>⽤于设置由matplotlib⽣成图表的轴的属性。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_axes</span>(<span class="params">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class="line">	<span class="string">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class="line">	axes.set_xlabel(xlabel)</span><br><span class="line">	axes.set_ylabel(ylabel)</span><br><span class="line">	axes.set_xscale(xscale)</span><br><span class="line">	axes.set_yscale(yscale)</span><br><span class="line">	axes.set_xlim(xlim)</span><br><span class="line">	axes.set_ylim(ylim)</span><br><span class="line">	<span class="keyword">if</span> legend:</span><br><span class="line">		axes.legend(legend)</span><br><span class="line">	axes.grid()</span><br></pre></td></tr></table></figure></li>
<li>定义⼀个plot函数来简洁地<strong>绘制多条曲线</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">X, Y=<span class="literal">None</span>, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">		 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>), axes=<span class="literal">None</span></span>):</span><br><span class="line">	<span class="string">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class="line">	<span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		legend = []</span><br><span class="line">		</span><br><span class="line">	set_figsize(figsize)</span><br><span class="line">	axes = axes <span class="keyword">if</span> axes <span class="keyword">else</span> d2l.plt.gca()</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 如果X有⼀个轴，输出True</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">has_one_axis</span>(<span class="params">X</span>):</span><br><span class="line">		<span class="keyword">return</span> (<span class="built_in">hasattr</span>(X, <span class="string">&quot;ndim&quot;</span>) <span class="keyword">and</span> X.ndim == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>)</span><br><span class="line">				<span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(X[<span class="number">0</span>], <span class="string">&quot;__len__&quot;</span>))</span><br><span class="line">				</span><br><span class="line">	<span class="keyword">if</span> has_one_axis(X):</span><br><span class="line">		X = [X]</span><br><span class="line">	<span class="keyword">if</span> Y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		X, Y = [[]] * <span class="built_in">len</span>(X), X</span><br><span class="line">	<span class="keyword">elif</span> has_one_axis(Y):</span><br><span class="line">		Y = [Y]</span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(X) != <span class="built_in">len</span>(Y):</span><br><span class="line">		X = X * <span class="built_in">len</span>(Y)</span><br><span class="line">	axes.cla()</span><br><span class="line">	<span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, fmts):</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">len</span>(x):</span><br><span class="line">			axes.plot(x, y, fmt)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			axes.plot(y, fmt)</span><br><span class="line">	set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="2-4-2-偏导数"><a href="#2-4-2-偏导数" class="headerlink" title="2.4.2 偏导数"></a>2.4.2 偏导数</h2><p>$$ \frac{\partial y}{\partial x_i} &#x3D; \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.$$</p>
<ul>
<li>对于偏导数的表示，以下是等价的：<br>$$\frac{\partial y}{\partial x_i} &#x3D; \frac{\partial f}{\partial x_i} &#x3D; f_{x_i} &#x3D; f_i &#x3D; D_i f &#x3D; D_{x_i} f.$$</li>
</ul>
<h2 id="2-4-3-梯度"><a href="#2-4-3-梯度" class="headerlink" title="2.4.3 梯度"></a>2.4.3 梯度</h2><ul>
<li>设函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$的<strong>输入是一个$n$维向量$\mathbf{x}&#x3D;[x_1,x_2,\ldots,x_n]^\top$<strong>，并且</strong>输出是一个标量</strong>。</li>
<li>函数$f(\mathbf{x})$相对于$\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:<br>$$\nabla_{\mathbf{x}} f(\mathbf{x}) &#x3D; \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top$$</li>
<li>其中$\nabla_{\mathbf{x}} f(\mathbf{x})$通常在没有歧义时被$\nabla f(\mathbf{x})$取代。</li>
<li>假设$\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:<ol>
<li>对于所有$\mathbf{A} \in \mathbb{R}^{m \times n}$，都有$\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} &#x3D; \mathbf{A}^\top$</li>
<li>对于所有$\mathbf{A} \in \mathbb{R}^{n \times m}$，都有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A}  &#x3D; \mathbf{A}$</li>
<li>对于所有$\mathbf{A} \in \mathbb{R}^{n \times n}$，都有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x}  &#x3D; (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$</li>
<li>$\nabla_{\mathbf{x}} |\mathbf{x} |^2 &#x3D; \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} &#x3D; 2\mathbf{x}$</li>
</ol>
</li>
<li>同样，对于任何矩阵$\mathbf{X}$，都有$\nabla_{\mathbf{X}} |\mathbf{X} |_F^2 &#x3D; 2\mathbf{X}$。</li>
</ul>
<h2 id="2-4-4-链式法则"><a href="#2-4-4-链式法则" class="headerlink" title="2.4.4 链式法则"></a>2.4.4 链式法则</h2><h2 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h2><h3 id="1-绘制函数-y-f-x-x-3-frac-1-x-和其在-x-1-处切线的图像。"><a href="#1-绘制函数-y-f-x-x-3-frac-1-x-和其在-x-1-处切线的图像。" class="headerlink" title="1.绘制函数$y &#x3D; f(x) &#x3D; x^3 - \frac{1}{x}$和其在$x &#x3D; 1$处切线的图像。"></a>1.绘制函数$y &#x3D; f(x) &#x3D; x^3 - \frac{1}{x}$和其在$x &#x3D; 1$处切线的图像。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x ** <span class="number">3</span> - <span class="number">1</span>/x</span><br><span class="line">    </span><br><span class="line">x = np.arange(<span class="number">0.1</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">plot(x, [f(x), <span class="number">4</span>*x-<span class="number">4</span>], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;f(x)&#x27;</span>, legend=[<span class="string">&#x27;f(x)&#x27;</span>, <span class="string">&#x27;Tangent line (x=1)&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h3 id="2-求函数-f-mathbf-x-3x-1-2-5e-x-2-的梯度。"><a href="#2-求函数-f-mathbf-x-3x-1-2-5e-x-2-的梯度。" class="headerlink" title="2.求函数$f(\mathbf{x}) &#x3D; 3x_1^2 + 5e^{x_2}$的梯度。"></a>2.求函数$f(\mathbf{x}) &#x3D; 3x_1^2 + 5e^{x_2}$的梯度。</h3><p>$$<br>\nabla_{\mathbf{x}} f(\mathbf{x}) &#x3D; [6x_1, 5e^{x_2}]^\top<br>$$</p>
<h3 id="3-函数-f-mathbf-x-mathbf-x-2-的梯度是什么？"><a href="#3-函数-f-mathbf-x-mathbf-x-2-的梯度是什么？" class="headerlink" title="3.函数$f(\mathbf{x}) &#x3D; |\mathbf{x}|_2$的梯度是什么？"></a>3.函数$f(\mathbf{x}) &#x3D; |\mathbf{x}|_2$的梯度是什么？</h3><ul>
<li><h4 id="二范数定义如下"><a href="#二范数定义如下" class="headerlink" title="二范数定义如下"></a>二范数定义如下</h4></li>
</ul>
<p>$$<br>|\mathbf{x}|<em>2&#x3D;\sqrt{\sum</em>{i&#x3D;1}^n x_i^2}<br>$$</p>
<ul>
<li><h4 id="证明如下"><a href="#证明如下" class="headerlink" title="证明如下"></a>证明如下</h4></li>
</ul>
<p>$$<br>\begin{equation}<br>\begin{aligned}<br>\nabla f(\mathbf{x}) &amp;&#x3D; \nabla |\mathbf{x}|<em>2 \<br>&amp; &#x3D;\nabla \sqrt{\sum</em>{i&#x3D;1}^n x_i^2} \<br>&amp; &#x3D;\bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top \<br>&amp; &#x3D;\bigg[\frac{\partial \sqrt{\sum_{i&#x3D;1}^n x_i^2} }{\partial x_1}, \frac{\partial \sqrt{\sum_{i&#x3D;1}^n x_i^2} }{\partial x_2}, \ldots, \frac{\partial \sqrt{\sum_{i&#x3D;1}^n x_i^2} }{\partial x_n}\bigg]^\top  \<br>&amp; &#x3D; \frac{1}{2|\mathbf{x}|_2}\bigg[2(x_1), 2(x_2), \ldots, 2(x_n)\bigg]^\top \<br>&amp; &#x3D; \frac{1}{|\mathbf{x}|_2}\bigg[x_1, x_2, \ldots, x_n\bigg]^\top \<br>&amp; &#x3D; \frac{\mathbf{x}}{|\mathbf{x}|_2}<br>\end{aligned}<br>\end{equation}<br>$$</p>
<h3 id="4-尝试写出函数-u-f-x-y-z-，其中-x-x-a-b-，-y-y-a-b-，-z-z-a-b-的链式法则。"><a href="#4-尝试写出函数-u-f-x-y-z-，其中-x-x-a-b-，-y-y-a-b-，-z-z-a-b-的链式法则。" class="headerlink" title="4.尝试写出函数$u &#x3D; f(x, y, z)$，其中$x &#x3D; x(a, b)$，$y &#x3D; y(a, b)$，$z &#x3D; z(a, b)$的链式法则。"></a>4.尝试写出函数$u &#x3D; f(x, y, z)$，其中$x &#x3D; x(a, b)$，$y &#x3D; y(a, b)$，$z &#x3D; z(a, b)$的链式法则。</h3><p>$$<br>\begin{align}<br>\frac{\partial u}{\partial a} &#x3D; \frac{\partial u}{\partial x}\frac{\partial x}{\partial a}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial a}+\frac{\partial u}{\partial z}\frac{\partial z}{\partial a} \<br>\frac{\partial u}{\partial a} &#x3D; \frac{\partial u}{\partial x}\frac{\partial x}{\partial a}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial a}+\frac{\partial u}{\partial z}\frac{\partial z}{\partial a}<br>\end{align}<br>$$</p>
<h1 id="2-5-自动微分"><a href="#2-5-自动微分" class="headerlink" title="2.5 自动微分"></a>2.5 自动微分</h1><h2 id="2-5-0-深度学习框架"><a href="#2-5-0-深度学习框架" class="headerlink" title="2.5.0 深度学习框架"></a>2.5.0 深度学习框架</h2><ul>
<li>通过⾃动计算导数，即⾃动微分（automatic differentiation）来加快求导。</li>
<li>根据设计好的模型，系统会构建⼀个<strong>计算图（computational graph）</strong>，来<strong>跟踪计算是哪些数据通过哪些操作组合起来产⽣输出。</strong></li>
<li>⾃动微分使系统能够随后<strong>反向传播梯度</strong>。这⾥，<strong>反向传播（backpropagate）</strong> 意味着<strong>跟踪整个计算图，填充关于每个参数的偏导数。</strong></li>
</ul>
<h2 id="2-5-1-⼀个简单的例⼦"><a href="#2-5-1-⼀个简单的例⼦" class="headerlink" title="2.5.1 ⼀个简单的例⼦"></a>2.5.1 ⼀个简单的例⼦</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x.requires_grad_(<span class="literal">True</span>) <span class="comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class="line">x.grad <span class="comment"># 默认值是None</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 调用反向传播函数</span></span><br><span class="line">x.grad <span class="comment"># 对于x分量的梯度</span></span><br></pre></td></tr></table></figure>

<h2 id="2-5-2-⾮标量变量的反向传播"><a href="#2-5-2-⾮标量变量的反向传播" class="headerlink" title="2.5.2 ⾮标量变量的反向传播"></a>2.5.2 ⾮标量变量的反向传播</h2><ul>
<li>在默认情况下，<strong>PyTorch会累积梯度</strong>，需要清除之前的值<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line"><span class="comment"># y = x.sum()</span></span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="2-5-3-分离计算"><a href="#2-5-3-分离计算" class="headerlink" title="2.5.3 分离计算"></a>2.5.3 分离计算</h2><ul>
<li><h5 id="将某些计算移动到记录的计算图之外"><a href="#将某些计算移动到记录的计算图之外" class="headerlink" title="将某些计算移动到记录的计算图之外"></a>将某些计算移动到记录的计算图之外</h5><ul>
<li>eg: 假设y是作为x的函数计算的，⽽z则是作为y和x的函数计算的。想象⼀下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为⼀个常数</li>
<li><strong>分离y来返回⼀个新变量u，该变量与y具有相同的值，但丢弃计算图中如何计算y的任何信息</strong></li>
<li>梯度不会向后流经u到x<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach() <span class="comment"># 分离计算，设置u变量，但仅使用值，不考虑如何计算的，从而避免x的反向传播</span></span><br><span class="line">z = u * x</span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="2-5-4-Python控制流的梯度计算"><a href="#2-5-4-Python控制流的梯度计算" class="headerlink" title="2.5.4 Python控制流的梯度计算"></a>2.5.4 Python控制流的梯度计算</h2><ul>
<li>使⽤⾃动微分的⼀个好处是：即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调⽤），仍然可以计算得到的变量的梯度</li>
</ul>
<h2 id="练习-2"><a href="#练习-2" class="headerlink" title="练习"></a>练习</h2><ol>
<li><p><strong>为什么计算⼆阶导数⽐⼀阶导数的开销要更⼤？</strong></p>
<ul>
<li>二阶相当于再复用一阶导数的计算</li>
</ul>
</li>
<li><p><strong>在运⾏反向传播函数之后，⽴即再次运⾏它，看看会发⽣什么。</strong></p>
<ul>
<li><h5 id="会报错RunTimeError"><a href="#会报错RunTimeError" class="headerlink" title="会报错RunTimeError"></a>会报错RunTimeError</h5></li>
<li>两次backward<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231213154137.png" alt="xxx" style="zoom:25%;" /></center></li>
</ul>
</li>
<li><p><strong>在控制流的例⼦中，我们计算d关于a的导数，如果将变量a更改为随机向量或矩阵，会发⽣什么？</strong></p>
<ul>
<li><h5 id="会报错RunTimeError-1"><a href="#会报错RunTimeError-1" class="headerlink" title="会报错RunTimeError"></a>会报错RunTimeError</h5></li>
<li>只支持标量的grad<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231213153848.png" alt="xxx" style="zoom:25%;" /></center></li>
</ul>
</li>
<li><p><strong>重新设计⼀个求控制流梯度的例⼦，运⾏并分析结果。</strong></p>
<ul>
<li>暂无</li>
</ul>
</li>
<li><p><strong>使$f(x)&#x3D;\sin(x)$，绘制$f(x)$和$\frac{df(x)}{dx}$的图像，其中后者不使用$f’(x)&#x3D;\cos(x)$。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="2-6-概率"><a href="#2-6-概率" class="headerlink" title="2.6 概率"></a>2.6 概率</h1><h2 id="2-6-1-基本概率论"><a href="#2-6-1-基本概率论" class="headerlink" title="2.6.1 基本概率论"></a>2.6.1 基本概率论</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> multinomial</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 投骰子</span></span><br><span class="line">fair_probs = torch.ones([<span class="number">6</span>]) / <span class="number">6</span> <span class="comment"># 1-6的概率都是1/6</span></span><br><span class="line">multinomial.Multinomial(<span class="number">1</span>, fair_probs).sample() <span class="comment"># 1次的模拟结果 tensor</span></span><br><span class="line">multinomial.Multinomial(<span class="number">10</span>, fair_probs).sample() <span class="comment"># 10次的模拟结果 tensor</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">counts = multinomial.Multinomial(<span class="number">10</span>, fair_probs).sample((<span class="number">500</span>,))</span><br><span class="line">cum_counts = counts.cumsum(dim=<span class="number">0</span>)</span><br><span class="line">estimates = cum_counts / cum_counts.<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">d2l.set_figsize((<span class="number">6</span>, <span class="number">4.5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">	d2l.plt.plot(estimates[:, i].numpy(),</span><br><span class="line">	label=(<span class="string">&quot;P(die=&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>) + <span class="string">&quot;)&quot;</span>))</span><br><span class="line">d2l.plt.axhline(y=<span class="number">0.167</span>, color=<span class="string">&#x27;black&#x27;</span>, linestyle=<span class="string">&#x27;dashed&#x27;</span>)</span><br><span class="line">d2l.plt.gca().set_xlabel(<span class="string">&#x27;Groups of experiments&#x27;</span>)</span><br><span class="line">d2l.plt.gca().set_ylabel(<span class="string">&#x27;Estimated probability&#x27;</span>)</span><br><span class="line">d2l.plt.legend();</span><br></pre></td></tr></table></figure>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231215110512.png" alt="xxx" style="zoom:30%;" /></center>

<ul>
<li><h3 id="概率论公理"><a href="#概率论公理" class="headerlink" title="概率论公理"></a>概率论公理</h3><ol>
<li>概率非负性</li>
<li>P(S) &#x3D; 1 {S: Sample Space}</li>
<li>互斥事件概率为单个事件发生概率之和</li>
</ol>
</li>
<li><h3 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h3><ol>
<li>离散随机变量</li>
<li>连续随机变量</li>
</ol>
</li>
</ul>
<h2 id="2-6-2-处理多个随机变量"><a href="#2-6-2-处理多个随机变量" class="headerlink" title="2.6.2 处理多个随机变量"></a>2.6.2 处理多个随机变量</h2><ul>
<li><h3 id="联合概率-Joint-Probability"><a href="#联合概率-Joint-Probability" class="headerlink" title="联合概率 (Joint Probability)"></a>联合概率 (Joint Probability)</h3></li>
<li><h3 id="条件概率-Conditional-Probability"><a href="#条件概率-Conditional-Probability" class="headerlink" title="条件概率 (Conditional Probability)"></a>条件概率 (Conditional Probability)</h3></li>
</ul>
<p>$$P(B &#x3D; b | A &#x3D; a)$$</p>
<ul>
<li><h3 id="乘法法则-Multiplication-Rule"><a href="#乘法法则-Multiplication-Rule" class="headerlink" title="乘法法则 (Multiplication Rule)"></a>乘法法则 (Multiplication Rule)</h3></li>
</ul>
<p>$$<br>P(A;B) &#x3D; P(B|A)P(A)<br>$$</p>
<ul>
<li><h3 id="⻉叶斯定理-Bayes’-Theorem"><a href="#⻉叶斯定理-Bayes’-Theorem" class="headerlink" title="⻉叶斯定理 (Bayes’ Theorem)"></a>⻉叶斯定理 (Bayes’ Theorem)</h3></li>
</ul>
<p>$$<br>P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}<br>$$</p>
<ul>
<li><h3 id="边缘化-Marginalization"><a href="#边缘化-Marginalization" class="headerlink" title="边缘化 (Marginalization)"></a>边缘化 (Marginalization)</h3></li>
</ul>
<p>$$<br>P(B) &#x3D; \sum_{A}{P(A,B)}<br>$$</p>
<ul>
<li><h3 id="独⽴性-Independence"><a href="#独⽴性-Independence" class="headerlink" title="独⽴性 (Independence)"></a>独⽴性 (Independence)</h3></li>
</ul>
<p>$$<br>P(A,B) &#x3D; P(A)P(B)<br>$$</p>
<h2 id="2-6-3-期望和⽅差"><a href="#2-6-3-期望和⽅差" class="headerlink" title="2.6.3 期望和⽅差"></a>2.6.3 期望和⽅差</h2><ul>
<li><h3 id="期望-Expectation"><a href="#期望-Expectation" class="headerlink" title="期望 (Expectation)"></a>期望 (Expectation)</h3></li>
</ul>
<p>$$<br>E[X] &#x3D; \sum_x{x P(X&#x3D;x)} \qquad \text{离散随机变量}<br>$$<br>$$<br>E[X] &#x3D; \int_x{x P(X&#x3D;x)} \qquad \text{连续随机变量}<br>$$</p>
<ul>
<li><h3 id="方差-Variance"><a href="#方差-Variance" class="headerlink" title="方差 (Variance)"></a>方差 (Variance)</h3></li>
</ul>
<p>$$<br>Var[X] &#x3D; E[(X - E(X))^2] &#x3D; E(X^2) - E(X)^2<br>$$</p>
<h2 id="练习-3"><a href="#练习-3" class="headerlink" title="练习"></a>练习</h2><ol>
<li><strong>进行$m&#x3D;500$组实验，每组抽取$n&#x3D;10$个样本。改变$m$和$n$，观察和分析实验结果。</strong><ul>
<li>Center Limit Theorem 中心极限定理</li>
</ul>
</li>
<li><strong>给定两个概率为$P(\mathcal{A})$和$P(\mathcal{B})$的事件，计算$P(\mathcal{A} \cup \mathcal{B})$和$P(\mathcal{A} \cap \mathcal{B})$的上限和下限。（提示：使用<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Venn_diagram">友元图</a>来展示这些情况。)</strong><br>$$<br>max{({P(\mathcal{A}),P({\mathcal{B}}}))} \leq P(\mathcal{A} \cup \mathcal{B}) \leq P(\mathcal{A} )+P(\mathcal{B} )<br>$$<br>$$<br>0 \leq P(\mathcal{A} \cap \mathcal{B}) \leq min{({P(\mathcal{A}),P({\mathcal{B}}}))} )<br>$$</li>
<li><strong>假设我们有一系列随机变量，例如$A$、$B$和$C$，其中$B$只依赖于$A$，而$C$只依赖于$B$，能简化联合概率$P(A, B, C)$吗？（提示：这是一个<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Markov_chain">马尔可夫链</a>。)</strong><br>$$<br>\begin{equation}<br>\begin{aligned}<br>P(A,B,C) &amp;&#x3D; P(C|B)*P(B) \<br>&amp;&#x3D;P(C|B)*P(B|A)*P(A)<br>\end{aligned}<br>\end{equation}<br>$$</li>
<li><strong>在2.6.2节中，第一个测试更准确。为什么不运行第一个测试两次，而是同时运行第一个和第二个测试?</strong><ul>
<li>也许是成本问题，只需要第二次的不精准测试，也可以大幅度提高推理在阳性反应下，患者真实患病的概率。同时可以避免，同种测试方法可能存在的误差和不为人知的错误。</li>
</ul>
</li>
</ol>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/blogavatar.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/blogavatar.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">望秋弥茂</div><div class="post-copyright__author_desc">松柏之质，经霜弥茂</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://www.wangqiumimao.ink/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://www.wangqiumimao.ink/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/')">Ch2 预备知识</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://www.wangqiumimao.ink/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=Ch2 预备知识&amp;url=https://www.wangqiumimao.ink/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/&amp;pic=https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231207195728.png" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.wangqiumimao.ink" target="_blank">望秋弥茂</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/Deep-Learning/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>Deep Learning<span class="tagsPageCount">1</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20240603160909.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/11/29/%E5%9F%BA%E4%BA%8E%E8%A7%86%E8%A7%A6%E8%9E%8D%E5%90%88%E7%9A%84%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8A%93%E5%8F%96%E6%BB%91%E5%8A%A8%E6%A3%80%E6%B5%8B%20--%20%E5%B4%94%E5%B0%91%E4%BC%9F/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231128224401.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">基于视触融合的机器人抓取滑动检测 -- 崔少伟</div></div></a></div><div class="next-post pull-right"><a href="/2024/03/06/(RSS%2020)%20Event-driven%20visual-tactile%20sensing%20and%20learning%20for%20robots/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20240306164612.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">(RSS 20) Event-driven visual-tactile sensing and learning for robots</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/blogavatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2021/01/15/568350965a98c.png" ait="status"/></div></div><div class="author-info__description">少年烈马狂歌的心，青年纵横捭阖的梦</div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">望秋弥茂</h1><div class="author-info__desc">松柏之质，经霜弥茂</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/zahir-w" target="_blank" title="Github"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-a-1834841_brand_github_logo_network_social_icon"></use></svg></a><a class="social-icon faa-parent animated-hover" href="https://www.zhihu.com/people/wang-zi-han-30-29" target="_blank" title="Zhihu"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-Group51"></use></svg></a></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2-1%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">2.1数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-5-%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98"><span class="toc-number">1.1.</span> <span class="toc-text">2.1.5 节省内存</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">2.2 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-1-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.1.</span> <span class="toc-text">2.2.1 读取数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-2-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">2.2.</span> <span class="toc-text">2.2.2 处理缺失值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-3-%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%BC%A0%E9%87%8F%E6%A0%BC%E5%BC%8F"><span class="toc-number">2.3.</span> <span class="toc-text">2.2.3 转换为张量格式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">2.3 线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-2-%E5%90%91%E9%87%8F"><span class="toc-number">3.1.</span> <span class="toc-text">2.3.2 向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-3-%E7%9F%A9%E9%98%B5"><span class="toc-number">3.2.</span> <span class="toc-text">2.3.3 矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-4-%E5%BC%A0%E9%87%8F"><span class="toc-number">3.3.</span> <span class="toc-text">2.3.4 张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-5-%E5%BC%A0%E9%87%8F%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%A7%E8%B4%A8"><span class="toc-number">3.4.</span> <span class="toc-text">2.3.5 张量算法的基本性质</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-6-%E9%99%8D%E7%BB%B4"><span class="toc-number">3.5.</span> <span class="toc-text">2.3.6 降维</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C"><span class="toc-number">3.5.1.</span> <span class="toc-text">非降维求和</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-7-%E7%82%B9%E7%A7%AF"><span class="toc-number">3.6.</span> <span class="toc-text">2.3.7 点积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-8-%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-number">3.7.</span> <span class="toc-text">2.3.8 矩阵-向量积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-9-%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-number">3.8.</span> <span class="toc-text">2.3.9 矩阵-矩阵乘法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-10-%E8%8C%83%E6%95%B0"><span class="toc-number">3.9.</span> <span class="toc-text">2.3.10 范数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-number">4.</span> <span class="toc-text">练习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%AF%81%E6%98%8E%E2%BC%80%E4%B8%AA%E7%9F%A9%E9%98%B5-A-%E7%9A%84%E8%BD%AC%E7%BD%AE%E7%9A%84%E8%BD%AC%E7%BD%AE%E6%98%AFA%EF%BC%8C%E5%8D%B3-A-T-T-A"><span class="toc-number">4.0.1.</span> <span class="toc-text">1.证明⼀个矩阵 $A$ 的转置的转置是A，即 $(A^T)^T &#x3D; A$</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%AF%8F%E4%B8%80%E4%B8%AA%E5%85%83%E7%B4%A0%E5%AE%9A%E4%B9%89%E6%98%93%E8%AF%81"><span class="toc-number">4.0.1.1.</span> <span class="toc-text">对于每一个元素定义易证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BB%99%E5%87%BA%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5-A-%E5%92%8C-B-%EF%BC%8C%E8%AF%81%E6%98%8E%E2%80%9C%E5%AE%83%E4%BB%AC%E8%BD%AC%E7%BD%AE%E7%9A%84%E5%92%8C%E2%80%9D%E7%AD%89%E4%BA%8E%E2%80%9C%E5%AE%83%E4%BB%AC%E5%92%8C%E7%9A%84%E8%BD%AC%E7%BD%AE%E2%80%9D%EF%BC%8C%E5%8D%B3-A-T-B-T-A-B-T"><span class="toc-number">4.0.2.</span> <span class="toc-text">2.给出两个矩阵 $A$ 和 $B$ ，证明“它们转置的和”等于“它们和的转置”，即$A^T+B^T&#x3D;(A+B)^T$</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%AF%8F%E4%B8%80%E4%B8%AA%E5%85%83%E7%B4%A0%E5%AE%9A%E4%B9%89%E6%98%93%E8%AF%81-1"><span class="toc-number">4.0.2.1.</span> <span class="toc-text">对于每一个元素定义易证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%BB%99%E5%AE%9A%E4%BB%BB%E6%84%8F%E2%BD%85%E9%98%B5-A-%EF%BC%8C-A-A-T-%E6%80%BB%E6%98%AF%E5%AF%B9%E7%A7%B0%E7%9A%84%E5%90%97-%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="toc-number">4.0.3.</span> <span class="toc-text">3.给定任意⽅阵 $A$ ， $A+A^T$ 总是对称的吗?为什么?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%AF%8F%E4%B8%80%E4%B8%AA%E5%85%83%E7%B4%A0%E5%AE%9A%E4%B9%89%E6%98%93%E8%AF%81-2"><span class="toc-number">4.0.3.1.</span> <span class="toc-text">对于每一个元素定义易证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%9C%AC%E8%8A%82%E4%B8%AD%E5%AE%9A%E4%B9%89%E4%BA%86%E5%BD%A2%E7%8A%B6-2-3-4-%E7%9A%84%E5%BC%A0%E9%87%8FX%E3%80%82len-X-%E7%9A%84%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.0.4.</span> <span class="toc-text">4.本节中定义了形状(2; 3; 4)的张量X。len(X)的输出结果是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%AF%B9%E4%BA%8E%E4%BB%BB%E6%84%8F%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BC%A0%E9%87%8FX-len-X-%E6%98%AF%E5%90%A6%E6%80%BB%E6%98%AF%E5%AF%B9%E5%BA%94%E4%BA%8EX%E7%89%B9%E5%AE%9A%E8%BD%B4%E7%9A%84%E2%BB%93%E5%BA%A6-%E8%BF%99%E4%B8%AA%E8%BD%B4%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">4.0.5.</span> <span class="toc-text">5.对于任意形状的张量X, len(X)是否总是对应于X特定轴的⻓度?这个轴是什么?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E5%BA%94shape%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E8%BD%B4%E7%9A%84%E6%95%B0%E7%9B%AE%EF%BC%8Caxis-0"><span class="toc-number">4.0.5.1.</span> <span class="toc-text">对应shape的第一个轴的数目，axis&#x3D;0</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%BF%90%E2%BE%8FA-A-sum-axis-1-%EF%BC%8C%E7%9C%8B%E7%9C%8B%E4%BC%9A%E5%8F%91%E2%BD%A3%E4%BB%80%E4%B9%88%E3%80%82%E8%AF%B7%E5%88%86%E6%9E%90%E2%BC%80%E4%B8%8B%E5%8E%9F%E5%9B%A0%EF%BC%9F"><span class="toc-number">4.0.6.</span> <span class="toc-text">6.运⾏A&#x2F;A.sum(axis&#x3D;1)，看看会发⽣什么。请分析⼀下原因？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E8%80%83%E8%99%91%E2%BC%80%E4%B8%AA%E5%85%B7%E6%9C%89%E5%BD%A2%E7%8A%B6-2-3-4-%E7%9A%84%E5%BC%A0%E9%87%8F%EF%BC%8C%E5%9C%A8%E8%BD%B40%E3%80%811%E3%80%812%E4%B8%8A%E7%9A%84%E6%B1%82%E5%92%8C%E8%BE%93%E5%87%BA%E6%98%AF%E4%BB%80%E4%B9%88%E5%BD%A2%E7%8A%B6"><span class="toc-number">4.0.7.</span> <span class="toc-text">7.考虑⼀个具有形状(2; 3; 4)的张量，在轴0、1、2上的求和输出是什么形状?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E7%BB%B4%E5%BC%A0%E9%87%8F%E5%8F%AF%E4%BB%A5%E8%A7%86%E4%BD%9C%E4%B8%80%E4%B8%AA%E7%A9%BA%E9%97%B4%E7%9F%A9%E9%98%B5%EF%BC%8C%E7%84%B6%E5%90%8E%E5%88%86%E5%88%AB%E6%B2%BF%E7%9D%80axis%EF%BC%88%E7%AC%AC%E4%B8%80%E4%B8%AA0%EF%BC%8C%E4%BE%9D%E6%AC%A1%E9%80%92%E5%A2%9E%EF%BC%89%E8%BF%9B%E8%A1%8C%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C%EF%BC%8C%E5%87%8F%E5%B0%91%E7%9A%84%E6%98%AF%E5%AF%B9%E5%BA%94axis%E6%96%B9%E5%90%91%E7%9A%84%E7%BB%B4%E5%BA%A6%E3%80%82"><span class="toc-number">4.0.7.1.</span> <span class="toc-text">三维张量可以视作一个空间矩阵，然后分别沿着axis（第一个0，依次递增）进行降维求和，减少的是对应axis方向的维度。</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E4%B8%BAlinalg-norm%E5%87%BD%E6%95%B0%E6%8F%90%E4%BE%9B3%E4%B8%AA%E6%88%96%E6%9B%B4%E5%A4%9A%E8%BD%B4%E7%9A%84%E5%BC%A0%E9%87%8F%EF%BC%8C%E5%B9%B6%E8%A7%82%E5%AF%9F%E5%85%B6%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E4%BB%BB%E6%84%8F%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BC%A0%E9%87%8F%E8%BF%99%E4%B8%AA%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E5%BE%97%E5%88%B0%E4%BB%80%E4%B9%88"><span class="toc-number">4.0.8.</span> <span class="toc-text">8.为linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AC%E8%B4%A8%E5%B0%B1%E6%98%AF%E5%9C%A8%E6%B1%82%E6%AF%8F%E4%B8%AA%E5%85%83%E7%B4%A0%E7%9A%84%E5%B9%B3%E6%96%B9%E5%92%8C%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9%E7%BB%93%E6%9E%9C%E3%80%82"><span class="toc-number">4.0.8.1.</span> <span class="toc-text">本质就是在求每个元素的平方和的平方根结果。</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-4-%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-number">5.</span> <span class="toc-text">2.4 微积分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E6%8B%9F%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BB%BB%E5%8A%A1%E5%88%86%E8%A7%A3%E4%B8%BA%E4%B8%A4%E4%B8%AA%E5%85%B3%E9%94%AE%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="toc-number">5.0.1.</span> <span class="toc-text">将拟合模型的任务分解为两个关键问题：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-1-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86"><span class="toc-number">5.1.</span> <span class="toc-text">2.4.1 导数与微分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-2-%E5%81%8F%E5%AF%BC%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">2.4.2 偏导数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-3-%E6%A2%AF%E5%BA%A6"><span class="toc-number">5.3.</span> <span class="toc-text">2.4.3 梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-4-%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-number">5.4.</span> <span class="toc-text">2.4.4 链式法则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-1"><span class="toc-number">5.5.</span> <span class="toc-text">练习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%BB%98%E5%88%B6%E5%87%BD%E6%95%B0-y-f-x-x-3-frac-1-x-%E5%92%8C%E5%85%B6%E5%9C%A8-x-1-%E5%A4%84%E5%88%87%E7%BA%BF%E7%9A%84%E5%9B%BE%E5%83%8F%E3%80%82"><span class="toc-number">5.5.1.</span> <span class="toc-text">1.绘制函数$y &#x3D; f(x) &#x3D; x^3 - \frac{1}{x}$和其在$x &#x3D; 1$处切线的图像。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B1%82%E5%87%BD%E6%95%B0-f-mathbf-x-3x-1-2-5e-x-2-%E7%9A%84%E6%A2%AF%E5%BA%A6%E3%80%82"><span class="toc-number">5.5.2.</span> <span class="toc-text">2.求函数$f(\mathbf{x}) &#x3D; 3x_1^2 + 5e^{x_2}$的梯度。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%87%BD%E6%95%B0-f-mathbf-x-mathbf-x-2-%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.5.3.</span> <span class="toc-text">3.函数$f(\mathbf{x}) &#x3D; |\mathbf{x}|_2$的梯度是什么？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E8%8C%83%E6%95%B0%E5%AE%9A%E4%B9%89%E5%A6%82%E4%B8%8B"><span class="toc-number">5.5.3.1.</span> <span class="toc-text">二范数定义如下</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%81%E6%98%8E%E5%A6%82%E4%B8%8B"><span class="toc-number">5.5.3.2.</span> <span class="toc-text">证明如下</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%B0%9D%E8%AF%95%E5%86%99%E5%87%BA%E5%87%BD%E6%95%B0-u-f-x-y-z-%EF%BC%8C%E5%85%B6%E4%B8%AD-x-x-a-b-%EF%BC%8C-y-y-a-b-%EF%BC%8C-z-z-a-b-%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E3%80%82"><span class="toc-number">5.5.4.</span> <span class="toc-text">4.尝试写出函数$u &#x3D; f(x, y, z)$，其中$x &#x3D; x(a, b)$，$y &#x3D; y(a, b)$，$z &#x3D; z(a, b)$的链式法则。</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-5-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">6.</span> <span class="toc-text">2.5 自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="toc-number">6.1.</span> <span class="toc-text">2.5.0 深度学习框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-1-%E2%BC%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E2%BC%A6"><span class="toc-number">6.2.</span> <span class="toc-text">2.5.1 ⼀个简单的例⼦</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-2-%E2%BE%AE%E6%A0%87%E9%87%8F%E5%8F%98%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">6.3.</span> <span class="toc-text">2.5.2 ⾮标量变量的反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-3-%E5%88%86%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">6.4.</span> <span class="toc-text">2.5.3 分离计算</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B0%86%E6%9F%90%E4%BA%9B%E8%AE%A1%E7%AE%97%E7%A7%BB%E5%8A%A8%E5%88%B0%E8%AE%B0%E5%BD%95%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B9%8B%E5%A4%96"><span class="toc-number">6.4.0.0.1.</span> <span class="toc-text">将某些计算移动到记录的计算图之外</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-4-Python%E6%8E%A7%E5%88%B6%E6%B5%81%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">6.5.</span> <span class="toc-text">2.5.4 Python控制流的梯度计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-2"><span class="toc-number">6.6.</span> <span class="toc-text">练习</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%9A%E6%8A%A5%E9%94%99RunTimeError"><span class="toc-number">6.6.0.0.1.</span> <span class="toc-text">会报错RunTimeError</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%9A%E6%8A%A5%E9%94%99RunTimeError-1"><span class="toc-number">6.6.0.0.2.</span> <span class="toc-text">会报错RunTimeError</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-6-%E6%A6%82%E7%8E%87"><span class="toc-number">7.</span> <span class="toc-text">2.6 概率</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="toc-number">7.1.</span> <span class="toc-text">2.6.1 基本概率论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E5%85%AC%E7%90%86"><span class="toc-number">7.1.1.</span> <span class="toc-text">概率论公理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="toc-number">7.1.2.</span> <span class="toc-text">随机变量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-2-%E5%A4%84%E7%90%86%E5%A4%9A%E4%B8%AA%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="toc-number">7.2.</span> <span class="toc-text">2.6.2 处理多个随机变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87-Joint-Probability"><span class="toc-number">7.2.1.</span> <span class="toc-text">联合概率 (Joint Probability)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87-Conditional-Probability"><span class="toc-number">7.2.2.</span> <span class="toc-text">条件概率 (Conditional Probability)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B9%98%E6%B3%95%E6%B3%95%E5%88%99-Multiplication-Rule"><span class="toc-number">7.2.3.</span> <span class="toc-text">乘法法则 (Multiplication Rule)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%BB%89%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86-Bayes%E2%80%99-Theorem"><span class="toc-number">7.2.4.</span> <span class="toc-text">⻉叶斯定理 (Bayes’ Theorem)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%B9%E7%BC%98%E5%8C%96-Marginalization"><span class="toc-number">7.2.5.</span> <span class="toc-text">边缘化 (Marginalization)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%AC%E2%BD%B4%E6%80%A7-Independence"><span class="toc-number">7.2.6.</span> <span class="toc-text">独⽴性 (Independence)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-3-%E6%9C%9F%E6%9C%9B%E5%92%8C%E2%BD%85%E5%B7%AE"><span class="toc-number">7.3.</span> <span class="toc-text">2.6.3 期望和⽅差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%9F%E6%9C%9B-Expectation"><span class="toc-number">7.3.1.</span> <span class="toc-text">期望 (Expectation)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E5%B7%AE-Variance"><span class="toc-number">7.3.2.</span> <span class="toc-text">方差 (Variance)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-3"><span class="toc-number">7.4.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/06/03/(RAL%2023)%20Orbit%20--%20A%20Unified%20Simulation%20Framework%20for%20Interactive%20Robot%20Learning%20Environments/" title="(RAL 23) Orbit - A Unified Simulation Framework for Interactive Robot Learning Environments"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20240603160909.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="(RAL 23) Orbit - A Unified Simulation Framework for Interactive Robot Learning Environments"/></a><div class="content"><a class="title" href="/2024/06/03/(RAL%2023)%20Orbit%20--%20A%20Unified%20Simulation%20Framework%20for%20Interactive%20Robot%20Learning%20Environments/" title="(RAL 23) Orbit - A Unified Simulation Framework for Interactive Robot Learning Environments">(RAL 23) Orbit - A Unified Simulation Framework for Interactive Robot Learning Environments</a><time datetime="2024-06-03T11:14:41.000Z" title="发表于 2024-06-03 19:14:41">2024-06-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/31/tactile_gym2/" title="(RAL 22) Tactile Gym 2.0 Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20240531220037.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="(RAL 22) Tactile Gym 2.0 Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch"/></a><div class="content"><a class="title" href="/2024/05/31/tactile_gym2/" title="(RAL 22) Tactile Gym 2.0 Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch">(RAL 22) Tactile Gym 2.0 Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch</a><time datetime="2024-05-31T13:57:00.000Z" title="发表于 2024-05-31 21:57:00">2024-05-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/29/Orbit/" title="Nvidia Orbit 仿真器环境配置"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20240529152748.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nvidia Orbit 仿真器环境配置"/></a><div class="content"><a class="title" href="/2024/05/29/Orbit/" title="Nvidia Orbit 仿真器环境配置">Nvidia Orbit 仿真器环境配置</a><time datetime="2024-05-29T12:16:10.000Z" title="发表于 2024-05-29 20:16:10">2024-05-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/06/(RSS%2020)%20Event-driven%20visual-tactile%20sensing%20and%20learning%20for%20robots/" title="(RSS 20) Event-driven visual-tactile sensing and learning for robots"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20240306164612.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="(RSS 20) Event-driven visual-tactile sensing and learning for robots"/></a><div class="content"><a class="title" href="/2024/03/06/(RSS%2020)%20Event-driven%20visual-tactile%20sensing%20and%20learning%20for%20robots/" title="(RSS 20) Event-driven visual-tactile sensing and learning for robots">(RSS 20) Event-driven visual-tactile sensing and learning for robots</a><time datetime="2024-03-06T13:01:29.000Z" title="发表于 2024-03-06 21:01:29">2024-03-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/" title="Ch2 预备知识"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/zahir-w/picgo/20231207195728.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ch2 预备知识"/></a><div class="content"><a class="title" href="/2023/12/07/Ch2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/" title="Ch2 预备知识">Ch2 预备知识</a><time datetime="2023-12-07T11:37:28.000Z" title="发表于 2023-12-07 19:37:28">2023-12-07</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2022 - 2024 By <a class="footer-bar-link" href="/" title="望秋弥茂" target="_blank">望秋弥茂</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">9</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 幽记</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-shounahe"></use></svg><span> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-shaixuan"></use></svg><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-biaoqian1"></use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 此间</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-taiyang"></use></svg><span> 友人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-wodeyouxiang"></use></svg><span> 夜话</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 观微</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/note/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-icon-rgb_jiqixuexisuanfayinqing"></use></svg><span> 格物</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-sf"></use></svg><span> 致知</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-dianzan"></use></svg><span> 本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xiangji"></use></svg><span> 光影</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Deep-Learning/" style="font-size: 0.88rem;">Deep Learning<sup>1</sup></a><a href="/tags/Imitation-Learning/" style="font-size: 0.88rem;">Imitation Learning<sup>1</sup></a><a href="/tags/Reinforcement-Learning/" style="font-size: 0.88rem;">Reinforcement Learning<sup>3</sup></a><a href="/tags/RobotMail/" style="font-size: 0.88rem;">RobotMail<sup>4</sup></a><a href="/tags/SNN/" style="font-size: 0.88rem;">SNN<sup>1</sup></a><a href="/tags/Simulator/" style="font-size: 0.88rem;">Simulator<sup>3</sup></a><a href="/tags/Slide-Detection/" style="font-size: 0.88rem;">Slide Detection<sup>1</sup></a><a href="/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/" style="font-size: 0.88rem;">博客搭建<sup>1</sup></a><a href="/tags/%E5%B0%8F%E7%BA%B8%E6%9D%A1/" style="font-size: 0.88rem;">小纸条<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="7537061859" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.4/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.cbd.int/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2022 By 安知鱼 V1.6.9",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 望秋弥茂 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.cbd.int/mathjax@3.2.2/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.cbd.int/mermaid@10.2.4/dist/mermaid.min.js').then(runMermaid)
  }

  anzhiyu.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "visitor@anheyu.com";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><script async src="https://at.alicdn.com/t/c/font_3611233_0d5med3ecomq.js?spm=a313x.manage_type_myprojects.i1.10.a0ee3a81wlj1Hb&amp;file=font_3611233_0d5med3ecomq.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>